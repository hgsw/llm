from langchain_model import Qwen2_LLM
import streamlit as st
import datetime
import pytz
import time

model_name_or_path = "/root/autodl-tmp/insult_code/qwen/Qwen2-7B-Instruct"


@st.cache_resource
def load_model():
    llm = Qwen2_LLM(model_name_or_path)
    return llm


def main():
    if "first_visit" not in st.session_state:
        st.session_state.first_visit = True
    else:
        st.session_state.first_visit = False
    if st.session_state.first_visit:
        # è·å–å½“å‰çš„ UTC æ—¶é—´
        utc_now = datetime.datetime.now(tz=pytz.utc)
        # å°† UTC æ—¶é—´è½¬æ¢ä¸ºåŒ—äº¬æ—¶é—´
        beijing_tz = pytz.timezone("Asia/Shanghai")
        beijing_now = utc_now.astimezone(beijing_tz)

        # è®¾ç½® session state ä¸­çš„ date_time ä¸ºåŒ—äº¬æ—¶é—´
        st.session_state.date_time = beijing_now
        # st.balloons()  # å¼¹å‡ºæ°”çƒ
    # åŠ è½½æ¨¡å‹
    llm = load_model()
    # å·¦ä¾§å¯¼èˆªæ 
    with st.sidebar:
        st.sidebar.header("Streamlit æ„å»º LLM æ¨¡å‹demo")
        st.caption(":fire: [Githubä»£ç åœ°å€](https://github.com/hgsw/llm.git)")
        # å¯¼èˆªæ æ˜¾ç¤ºæ—¥æœŸ
        st.sidebar.date_input("å½“å‰æ—¥æœŸï¼š", st.session_state.date_time.date())

        st.sidebar.subheader("æˆ‘å¯ä»¥å¸®ä½ è§£å†³ä»¥ä¸‹é—®é¢˜ï¼š")
        background = "æˆ‘æ˜¯ä¸€ä¸ªäººå·¥æ™ºèƒ½åŠ©æ‰‹ï¼Œå¯ä»¥é™ªä½ è§£å†³å„ç§é—®é¢˜ï¼Œ æ¯”å¦‚æ–‡æœ¬ç”Ÿæˆï¼Œæ–‡æœ¬æ¦‚æ‹¬ç­‰ã€‚"
        st.markdown(background)

        st.sidebar.subheader("å‚æ•°æ§åˆ¶ï¼š")
        # åˆ›å»ºä¸€ä¸ªæ»‘å—ï¼Œç”¨äºé€‰æ‹©æœ€å¤§é•¿åº¦ï¼ŒèŒƒå›´åœ¨0åˆ°1024ä¹‹é—´ï¼Œé»˜è®¤å€¼ä¸º512
        max_length = st.slider("Max input tokens", 0, 1024, 512, step=1)
        max_new_tokens = st.slider("Max output tokens", 0, 1024, 512, step=1)
        temperature = st.slider("Temperature", 0.0, 1.0, 0.7, step=0.1)
        # æ–¹æ¡†é£æ ¼çš„é€‰æ‹©æ ·å¼
        # temperature = st.number_input("Temperature", min_value=0.01, max_value=0.99, value=0.7, step=0.05)

        # æ¸…ç©ºä¼šè¯åˆ é™¤æ‰€æœ‰å†å²å¯¹è¯è®°å½•
        st.sidebar.subheader("å±é™©æ“ä½œï¼š")
        st.button("æ¸…ç©ºä¼šè¯", on_click=init_session)

    st.title("LLM Chat Robot")
    st.caption("ğŸš€ A streamlit chatbot demo, base langchain inference")

    # åˆå§‹åŒ–é—®å€™æ¶ˆæ¯
    if "messages" not in st.session_state:
        st.session_state.messages = []
        st.session_state.messages.append({"role": "assistant", "content": "ä½ å¥½ï¼Œæœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©ä½ å—ï¼Ÿ"})
    # æ˜¾ç¤ºæ‰€æœ‰å†å²æ¶ˆæ¯
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # æé—®æ¡†å’Œå¯¹è¯æ¨¡å—è®¾ç½®
    if prompt := st.chat_input("Ask anything"):
        prompt = check_len(prompt, max_length)
        st.chat_message("user").markdown(prompt)
        # ä¿å­˜å†å²
        st.session_state.messages.append({"role": "user", "content": prompt})
        # æ‰“å°æœºæ•ˆæœ
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            full_response = ""
            # è°ƒç”¨å¤§æ¨¡å‹è·å–ç»“æœ
            param = dict(max_length=max_length, max_new_tokens=max_new_tokens, temperature=temperature)
            print(f"parameter: {param}")
            response = generate(llm, prompt, **param)
            for trunk in list(response):
                full_response += trunk
                time.sleep(0.05)
                message_placeholder.markdown(full_response + "â–Œ")
            message_placeholder.markdown(full_response)

        st.session_state.messages.append({"role": "assistant", "content": full_response})
    # ä¿è¯åˆå§‹åŒ–é—®å€™è¯­æ— æ³•åˆ é™¤
    if len(st.session_state.messages) >= 2:
        deleted = st.session_state.messages[-1]
        st.button("åˆ é™¤æ¶ˆæ¯", on_click=delete_message, args={deleted["content"]})


def check_len(prompt, max_length=512):
    if len(prompt) > max_length:
        return f"è¾“å…¥æ–‡æœ¬è¶…å‡ºé™åˆ¶{max_length}ï¼Œè¯·é‡æ–°è¾“å…¥~"
    else:
        return prompt


def delete_message(message):
    for msg in st.session_state.messages:
        if msg["content"] == message:
            st.session_state.messages.remove(msg)
            break


def init_session():
    st.session_state.messages = []
    st.session_state.messages.append({"role": "assistant", "content": "ä½ å¥½ï¼Œæœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©ä½ å—ï¼Ÿ"})


def generate(llm, prompt, **param):
    response = llm(prompt, **param)
    return response


if __name__ == "__main__":
    main()
